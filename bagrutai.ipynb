{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BagrutAI â€” Fineâ€‘tuning an Israeli Civics (××–×¨×—×•×ª) â€œBagrut tutorâ€\n",
    "\n",
    "**Final high school project**\n",
    "\n",
    "- **Student**: *(fill in)*\n",
    "- **School / Class**: *(fill in)*\n",
    "- **Date**: *(fill in)*\n",
    "\n",
    "- **Goal**: Fineâ€‘tune a small language model to answer Israeli Civics (××–×¨×—×•×ª) Bagrutâ€‘style questions in **Hebrew**, with **short, factual** answers.\n",
    "- **Approach**: Combine **real** Bagrut Q/A with **synthetic** Q/A generated from the study material, then fineâ€‘tune using **LoRA**.\n",
    "\n",
    "## What this notebook does\n",
    "\n",
    "- **Part A â€” Synthetic data generation**: Generate additional Bagrutâ€‘style questions and answers from a civics study document (`civics.docx`).\n",
    "- **Part B â€” Dataset build**: Clean, normalize, and combine real + synthetic data into a single training dataset.\n",
    "- **Part C â€” Fineâ€‘tuning**: Fineâ€‘tune `Gemma 2 2B` (4â€‘bit) with LoRA using Unsloth.\n",
    "- **Part D â€” Evaluation**: Evaluate qualitatively on a **realâ€‘only** holdout set.\n",
    "\n",
    "## How to run (recommended order)\n",
    "\n",
    "1. *(Optional)* Run **Part A** if you want to regenerate synthetic data.\n",
    "2. Run **Part B/C** to build `master_training_data.jsonl` and fineâ€‘tune the model.\n",
    "3. Run **Part D** to compare **base vs fineâ€‘tuned** answers on `test_real.jsonl`.\n",
    "\n",
    "## Project files (JSONL)\n",
    "\n",
    "- `training_data.jsonl`: **Real** Bagrut questions & answers.\n",
    "- `synthetic_data_massive.jsonl`: **Synthetic** Q/A generated from the study document.\n",
    "- `master_training_data.jsonl`: The **reproducible training mix** written by this notebook.\n",
    "- `test_real.jsonl`: A **realâ€‘only holdout set** for final evaluation (never used in training).\n",
    "\n",
    "## Reproducibility\n",
    "\n",
    "- A fixed seed is used (`SEED = 3407`) so sampling and splits are repeatable.\n",
    "- The notebook writes out the exact training mix so results can be reproduced.\n",
    "\n",
    "## Responsible use\n",
    "\n",
    "- This model may still **make mistakes** or **hallucinate**; it should be used as a study aid.\n",
    "- Real exam questions may be copyrighted; avoid redistributing proprietary materials.\n",
    "\n",
    "## References (tools used)\n",
    "\n",
    "- Unsloth (LoRA fineâ€‘tuning)\n",
    "- Hugging Face `datasets`\n",
    "- TRL `SFTTrainer`\n",
    "\n",
    "> Tip for your report: include screenshots of dataset sizes, training/eval loss, and before/after answers on `test_real.jsonl`.\n",
    ""
   ],
   "id": "be2ead32"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "## Part A â€” Generate synthetic Bagrutâ€‘style Q/A from study material\n",
    "\n",
    "This section generates **additional practice questions** from a civics study document.\n",
    "\n",
    "### Inputs\n",
    "\n",
    "- **`civics.docx`**: a Word document containing the civics study material.\n",
    "- **Gemini API key**: stored in Colab Secrets as `GEMINI_API_KEY`.\n",
    "\n",
    "### Output\n",
    "\n",
    "- **`synthetic_data_massive.jsonl`**: one JSON object per line, formatted as:\n",
    "\n",
    "```json\n",
    "{\"messages\": [{\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"model\", \"content\": \"...\"}]}\n",
    "```\n",
    "\n",
    "### Quality guidelines (important)\n",
    "\n",
    "- Synthetic data should be **short, factual, and examâ€‘style**.\n",
    "- If the model returns invalid JSON, the code skips that chunk.\n",
    "- After generating synthetic data, we later **mix it with real data** (and keep a realâ€‘only test set) to reduce â€œAIâ€‘styleâ€ artifacts.\n"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "a95afd83"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 1. Install libraries\n",
    "%pip install google-genai python-docx -q\n",
    "\n",
    "from google import genai\n",
    "import docx\n",
    "import json\n",
    "import time\n",
    "from google.colab import userdata\n",
    "\n",
    "# 2. Connect to the API\n",
    "try:\n",
    "    GOOGLE_API_KEY = userdata.get('GEMINI_API_KEY')\n",
    "    client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "    print(\"âœ… Successfully connected to the API!\")\n",
    "except Exception as e:\n",
    "    print(\"âŒ Connection error. Make sure the key is defined in Secrets.\")\n",
    "    raise e\n",
    "\n",
    "# 3. Read the Word file and divide into chunks\n",
    "word_file_path = \"/content/civics.docx\" # Change this if your file name is different!\n",
    "\n",
    "print(f\"ğŸ“„ Reading the file {word_file_path}...\")\n",
    "try:\n",
    "    doc = docx.Document(word_file_path)\n",
    "except Exception as e:\n",
    "    raise FileNotFoundError(f\"Could not find the file '{word_file_path}'. Did you upload it to the sidebar?\")\n",
    "\n",
    "# Collect all text and divide into chunks\n",
    "paragraphs = [para.text for para in doc.paragraphs if para.text.strip()]\n",
    "full_text = \"\\n\".join(paragraphs)\n",
    "\n",
    "chunk_size = 4000 # Number of characters per chunk (about 1.5 to 2 pages)\n",
    "text_chunks = [full_text[i:i + chunk_size] for i in range(0, len(full_text), chunk_size)]\n",
    "\n",
    "print(f\"ğŸ“š The file was divided into {len(text_chunks)} chunks. Starting question generation!\")\n",
    "\n",
    "system_instruction = \"××ª×” ××•×¨×” ×¢×–×¨ ×œ××–×¨×—×•×ª ×©××›×™×Ÿ ×ª×œ××™×“×™× ×œ×‘×’×¨×•×ª ×‘×™×©×¨××œ.\"\n",
    "total_generated = 0\n",
    "\n",
    "# 4. Loop through each chunk and generate questions\n",
    "for index, chunk in enumerate(text_chunks):\n",
    "    print(f\"â³ Processing chunk {index + 1} out of {len(text_chunks)}...\")\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are an expert Israeli Civics teacher creating training data for an AI.\n",
    "    Read the following text and generate up to 10 official Bagrut-style questions and answers based ONLY on this text.\n",
    "    \n",
    "    The answers must be short, factual, and written in Hebrew.\n",
    "    \n",
    "    Format your output STRICTLY as a JSON list of objects, like this:\n",
    "    [\n",
    "      {{\"question\": \"...\", \"answer\": \"...\"}}\n",
    "    ]\n",
    "    \n",
    "    Text to use:\n",
    "    {chunk}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Call Gemini 2.5 Flash\n",
    "        response = client.models.generate_content(\n",
    "            model='gemini-2.5-flash',\n",
    "            contents=prompt\n",
    "        )\n",
    "        \n",
    "        # Clean and parse the JSON\n",
    "        json_text = response.text.strip().replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "        new_qa_pairs = json.loads(json_text)\n",
    "        \n",
    "        # Save the questions to a new file\n",
    "        with open(\"synthetic_data_massive.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
    "            for pair in new_qa_pairs:\n",
    "                user_input = f\"{system_instruction}\\n\\n{pair['question']}\"\n",
    "                entry = {\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"user\", \"content\": user_input},\n",
    "                        {\"role\": \"model\", \"content\": pair['answer']}\n",
    "                    ]\n",
    "                }\n",
    "                f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "        total_generated += len(new_qa_pairs)\n",
    "        print(f\"âœ… Added {len(new_qa_pairs)} questions (Total so far: {total_generated}).\")\n",
    "        \n",
    "        # A short 3-second pause to respect the API quota (Rate Limit)\n",
    "        time.sleep(3)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error in chunk {index + 1}, the model probably returned an invalid format. Skipping to the next chunk.\")\n",
    "\n",
    "print(f\"ğŸ‰ We are done! The file 'synthetic_data_massive.jsonl' is ready with {total_generated} questions and answers.\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "62e3367c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B/C â€” Build dataset + Fineâ€‘tune with LoRA (Unsloth)\n",
    "\n",
    "This section:\n",
    "\n",
    "1. Loads **real** (`training_data.jsonl`) and **synthetic** (`synthetic_data_massive.jsonl`) datasets.\n",
    "2. Cleans and normalizes message format (including fixing synthetic rows where the â€œsystemâ€ prompt was embedded inside the user message).\n",
    "3. Creates a **realâ€‘only holdout test set** (`test_real.jsonl`).\n",
    "4. Builds a controlled **train mix** (default: **80% real / 20% synthetic**) and saves it as `master_training_data.jsonl`.\n",
    "5. Fineâ€‘tunes a small instruction model using **4â€‘bit + LoRA**, which is practical on Google Colab GPUs.\n",
    "\n",
    "### Why Gemma 2 2B + LoRA?\n",
    "\n",
    "- **Small enough** to train on Colab.\n",
    "- **LoRA** updates only a small number of parameters, making training fast and memoryâ€‘efficient.\n",
    "- **4â€‘bit** quantization reduces VRAM usage while keeping good quality for an educational demo.\n",
    "\n",
    "### Key outputs\n",
    "\n",
    "- `outputs/lora_adapter/`: the trained LoRA adapter (can be reloaded later).\n",
    "- `test_real.jsonl`: the realâ€‘only holdout set used for final evaluation.\n"
   ],
   "id": "d68e4772"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# 1. INSTALL & SETUP\ntry:\n    import unsloth\nexcept ImportError:\n    print(\"â³ Installing Unsloth... (approx 2 mins)\")\n    %pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n    %pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n\nimport torch\nfrom unsloth import FastLanguageModel\nfrom datasets import Dataset, load_dataset\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments, EarlyStoppingCallback\nimport json\nimport os\nimport random\nimport hashlib\n\n# 2. LOAD MODEL (Switched to 2B for stability)\n# The 2B model is lighter and runs perfectly on Colab's standard GPU\nmax_seq_length = 2048\ndtype = None\nload_in_4bit = True\n\nprint(\"â³ Loading Gemma 2 (2B) model...\")\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/gemma-2-2b-it-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)\n\n# 3. PREPARE DATA\nprint(\"â³ Preparing dataset...\")\n\nREAL_FILE = \"training_data.jsonl\"\nSYNTH_FILE = \"synthetic_data_massive.jsonl\"\nMASTER_FILE = \"master_training_data.jsonl\"\n\nSYNTH_RATIO = 0.20           # target fraction of synthetic in training mix\nREAL_TEST_FRACTION = 0.10    # real-only holdout fraction\nSEED = 3407\n\nteacher_prefix = \"××ª×” ××•×¨×” ×¢×–×¨ ×œ××–×¨×—×•×ª\"\n\n\ndef read_jsonl(path):\n    rows = []\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            rows.append(json.loads(line))\n    return rows\n\n\ndef normalize_messages(messages):\n    if not isinstance(messages, list):\n        return None\n\n    sys = next((m.get(\"content\") for m in messages if isinstance(m, dict) and m.get(\"role\") == \"system\"), \"\")\n    usr = next((m.get(\"content\") for m in messages if isinstance(m, dict) and m.get(\"role\") == \"user\"), \"\")\n    mod = next(\n        (m.get(\"content\") for m in messages if isinstance(m, dict) and m.get(\"role\") in {\"model\", \"assistant\"}),\n        \"\",\n    )\n\n    # Synthetic file often embedded the system instruction in the user message.\n    if (not sys) and isinstance(usr, str):\n        u = usr.strip()\n        if u.startswith(teacher_prefix) and \"\\n\\n\" in u:\n            sys_candidate, rest = u.split(\"\\n\\n\", 1)\n            if sys_candidate.strip().startswith(teacher_prefix):\n                sys = sys_candidate.strip()\n                usr = rest.strip()\n\n    if not isinstance(usr, str) or not usr.strip() or not isinstance(mod, str) or not mod.strip():\n        return None\n\n    norm = []\n    if isinstance(sys, str) and sys.strip():\n        norm.append({\"role\": \"system\", \"content\": sys.strip()})\n    norm.append({\"role\": \"user\", \"content\": usr.strip()})\n    norm.append({\"role\": \"model\", \"content\": mod.strip()})\n    return norm\n\n\ndef dedup_rows(rows):\n    \"\"\"Remove duplicate Q&A pairs based on user+model content hash.\"\"\"\n    seen = set()\n    unique = []\n    for row in rows:\n        msgs = row.get(\"messages\", [])\n        usr = next((m.get(\"content\", \"\") for m in msgs if m.get(\"role\") == \"user\"), \"\")\n        mod = next((m.get(\"content\", \"\") for m in msgs if m.get(\"role\") in {\"model\", \"assistant\"}), \"\")\n        key = hashlib.md5((usr.strip() + mod.strip()).encode(\"utf-8\")).hexdigest()\n        if key not in seen:\n            seen.add(key)\n            unique.append(row)\n    removed = len(rows) - len(unique)\n    if removed > 0:\n        print(f\"  â†³ Dedup removed {removed} duplicate rows\")\n    return unique\n\n\ndef build_dataset_from_sources():\n    has_real = os.path.exists(REAL_FILE)\n    has_synth = os.path.exists(SYNTH_FILE)\n\n    if has_real and has_synth:\n        real_rows = read_jsonl(REAL_FILE)\n        synth_rows = read_jsonl(SYNTH_FILE)\n\n        real = []\n        synth = []\n\n        for r in real_rows:\n            msgs = normalize_messages(r.get(\"messages\"))\n            if msgs is not None:\n                real.append({\"messages\": msgs, \"source\": \"real\"})\n\n        for r in synth_rows:\n            msgs = normalize_messages(r.get(\"messages\"))\n            if msgs is not None:\n                synth.append({\"messages\": msgs, \"source\": \"synthetic\"})\n\n        if not real:\n            raise ValueError(\"Real dataset loaded but produced 0 usable rows after cleaning.\")\n\n        # Deduplicate each source\n        print(\"ğŸ” Deduplicating real data...\")\n        real = dedup_rows(real)\n        print(\"ğŸ” Deduplicating synthetic data...\")\n        synth = dedup_rows(synth)\n\n        rng = random.Random(SEED)\n\n        # Real-only test set\n        n_test = max(10, int(len(real) * REAL_TEST_FRACTION))\n        n_test = min(n_test, max(10, len(real) - 1))\n        test_real = rng.sample(real, n_test)\n        test_ids = {id(x) for x in test_real}\n        real_train = [x for x in real if id(x) not in test_ids]\n\n        # Keep all real_train; downsample synthetic to desired ratio\n        if synth and 0.0 < SYNTH_RATIO < 1.0:\n            target_synth = int(len(real_train) * (SYNTH_RATIO / (1.0 - SYNTH_RATIO)))\n            target_synth = max(0, min(target_synth, len(synth)))\n            synth_selected = rng.sample(synth, target_synth) if target_synth else []\n        else:\n            synth_selected = []\n\n        combined = real_train + synth_selected\n        rng.shuffle(combined)\n\n        # Save the real-only test set for final evaluation/demo\n        with open(\"test_real.jsonl\", \"w\", encoding=\"utf-8\") as f:\n            for row in test_real:\n                f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n\n        # Save combined training set for reproducibility\n        with open(MASTER_FILE, \"w\", encoding=\"utf-8\") as f:\n            for row in combined:\n                f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n\n        print(\n            \"âœ… Built combined dataset from sources:\\n\"\n            f\"  real usable:          {len(real)}\\n\"\n            f\"  synthetic usable:     {len(synth)}\\n\"\n            f\"  train mix (real):     {len(real_train)}\\n\"\n            f\"  train mix (synthetic):{len(synth_selected)}\\n\"\n            f\"  holdout (real-only):  {len(test_real)}\\n\"\n            f\"  saved: {MASTER_FILE}\\n\"\n            f\"  saved: test_real.jsonl\"\n        )\n\n        return Dataset.from_list(combined)\n\n    # Fallback to an already-combined file\n    for p in [MASTER_FILE, REAL_FILE]:\n        if os.path.exists(p):\n            print(f\"âœ… Using existing dataset file: {p}\")\n            return load_dataset(\"json\", data_files=p, split=\"train\")\n\n    raise FileNotFoundError(\n        f\"Could not find '{REAL_FILE}' and '{SYNTH_FILE}', and no fallback '{MASTER_FILE}'.\"\n    )\n\n\ndef format_chat_template_fixed(examples):\n    texts = []\n    for messages in examples[\"messages\"]:\n        if not isinstance(messages, list):\n            texts.append(\"\")\n            continue\n\n        sys = next((m.get(\"content\") for m in messages if isinstance(m, dict) and m.get(\"role\") == \"system\"), \"\")\n        usr = next((m.get(\"content\") for m in messages if isinstance(m, dict) and m.get(\"role\") == \"user\"), \"\")\n        mod = next((m.get(\"content\") for m in messages if isinstance(m, dict) and m.get(\"role\") == \"model\"), \"\")\n\n        user_input = f\"{sys}\\n\\n{usr}\" if sys else usr\n        if not isinstance(user_input, str) or not user_input.strip() or not isinstance(mod, str) or not mod.strip():\n            texts.append(\"\")\n            continue\n\n        conversation = tokenizer.apply_chat_template(\n            [\n                {\"role\": \"user\", \"content\": user_input.strip()},\n                {\"role\": \"model\", \"content\": mod.strip()},\n            ],\n            tokenize=False,\n            add_generation_prompt=False,\n        )\n        texts.append(conversation)\n\n    return {\"text\": texts}\n\nraw_dataset = build_dataset_from_sources()\ndataset = raw_dataset.map(format_chat_template_fixed, batched=True)\ndataset = dataset.filter(lambda x: bool(x.get(\"text\")))\n\nsplit_ds = dataset.train_test_split(test_size=0.05, seed=SEED)\ntrain_dataset = split_ds[\"train\"]\neval_dataset = split_ds[\"test\"]\n\nprint(f\"ğŸ“Š Train: {len(train_dataset)} | Eval: {len(eval_dataset)}\")\n\n# 4. SETUP LORA (increased rank for better capacity)\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 32,                    # increased from 16 for more capacity\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 64,           # 2x rank (common best practice)\n    lora_dropout = 0,\n    bias = \"none\",\n    use_gradient_checkpointing = \"unsloth\",\n    random_state = 3407,\n)\n\n# 5. TRAIN\nprint(\"ğŸš€ Starting Training...\")\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = train_dataset,\n    eval_dataset = eval_dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = False,\n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 10,              # increased warmup\n        max_steps = 120,                # doubled for better convergence\n        learning_rate = 2e-4,\n        fp16 = not torch.cuda.is_bf16_supported(),\n        bf16 = torch.cuda.is_bf16_supported(),\n        logging_steps = 1,\n        evaluation_strategy = \"steps\",\n        eval_steps = 15,\n        save_strategy = \"steps\",\n        save_steps = 30,\n        save_total_limit = 2,\n        load_best_model_at_end = True,  # keep the best checkpoint\n        metric_for_best_model = \"eval_loss\",\n        greater_is_better = False,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        max_grad_norm = 1.0,            # gradient clipping\n        lr_scheduler_type = \"cosine\",   # smoother LR decay\n        seed = 3407,\n        output_dir = \"outputs\",\n        report_to = \"none\",\n    ),\n)\n\ntrainer.train()\n\ntrainer.model.save_pretrained(\"outputs/lora_adapter\")\ntokenizer.save_pretrained(\"outputs/lora_adapter\")\n\nprint(\"ğŸ‰ Training Complete! Saved adapter to outputs/lora_adapter\")",
   "execution_count": null,
   "outputs": [],
   "id": "76b99d78"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D â€” Evaluation (realâ€‘only holdout)\n",
    "\n",
    "To make this a strong final project, we evaluate on a **realâ€‘only** set (`test_real.jsonl`) that was **never used for training**.\n",
    "\n",
    "### What to look for\n",
    "\n",
    "- **Correctness**: does the answer match the civics material?\n",
    "- **Bagrut style**: short, factual, uses proper terms (e.g., ×¢×§×¨×•× ×•×ª ×“××•×§×¨×˜×™×™×, ×—×•×§×™ ×™×¡×•×“).\n",
    "- **No hallucinations**: if unsure, the answer should be conservative.\n",
    "\n",
    "### Suggested writeâ€‘up structure (easy to grade)\n",
    "\n",
    "- **Data**: where each dataset came from (real vs synthetic), and why mixing helps.\n",
    "- **Method**: LoRA fineâ€‘tuning (why itâ€™s efficient), model choice, key hyperparameters.\n",
    "- **Evaluation**: realâ€‘only holdout + before/after examples.\n",
    "- **Limitations**: dataset size, synthetic artifacts, no guarantee of correctness.\n",
    "- **Future work**: more real questions, dedup, more question types (×§×˜×¢/××™×¨×•×¢), better refusal behavior.\n"
   ],
   "id": "aa6d4e8e"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Demo: compare base vs fine-tuned on real-only holdout\n# Run this AFTER training (requires outputs/lora_adapter and test_real.jsonl).\n\nimport json\nimport os\nimport random\nimport torch\n\nfrom peft import PeftModel\nfrom unsloth import FastLanguageModel\nfrom IPython.display import display, HTML\n\nBASE_MODEL_NAME = \"unsloth/gemma-2-2b-it-bnb-4bit\"\nADAPTER_DIR = \"outputs/lora_adapter\"\nHOLDOUT_FILE = \"test_real.jsonl\"\n\nN_SAMPLES = 10\nMAX_NEW_TOKENS = 200\nSEED = 3407\n\n\ndef load_holdout(path):\n    rows = []\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            rows.append(json.loads(line))\n    return rows\n\n\ndef extract_sys_user_ref(messages):\n    sys = \"\"\n    usr = \"\"\n    ref = \"\"\n    for m in messages:\n        if not isinstance(m, dict):\n            continue\n        if m.get(\"role\") == \"system\" and not sys:\n            sys = m.get(\"content\", \"\")\n        if m.get(\"role\") == \"user\" and not usr:\n            usr = m.get(\"content\", \"\")\n        if m.get(\"role\") in {\"model\", \"assistant\"} and not ref:\n            ref = m.get(\"content\", \"\")\n    return (sys or \"\").strip(), (usr or \"\").strip(), (ref or \"\").strip()\n\n\ndef generate_answer(model, tokenizer, sys, usr):\n    user_input = f\"{sys}\\n\\n{usr}\" if sys else usr\n    prompt = tokenizer.apply_chat_template(\n        [{\"role\": \"user\", \"content\": user_input}],\n        tokenize=False,\n        add_generation_prompt=True,\n    )\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n    with torch.no_grad():\n        out = model.generate(\n            **inputs,\n            max_new_tokens=MAX_NEW_TOKENS,\n            do_sample=False,\n        )\n\n    answer_tokens = out[0][inputs[\"input_ids\"].shape[-1] :]\n    return tokenizer.decode(answer_tokens, skip_special_tokens=True).strip()\n\n\n# Load holdout questions\nif not torch.cuda.is_available():\n    print(\"âš ï¸ GPU not detected. This will be slow on CPU.\")\n\nif not (os.path.exists(HOLDOUT_FILE) and os.path.exists(ADAPTER_DIR)):\n    raise FileNotFoundError(\n        f\"Missing '{HOLDOUT_FILE}' or '{ADAPTER_DIR}'. Run the training cell first.\"\n    )\n\nrows = load_holdout(HOLDOUT_FILE)\nrng = random.Random(SEED)\nrng.shuffle(rows)\nrows = rows[: min(N_SAMPLES, len(rows))]\n\n# Load base model\nbase_model, base_tokenizer = FastLanguageModel.from_pretrained(\n    model_name=BASE_MODEL_NAME,\n    max_seq_length=2048,\n    dtype=None,\n    load_in_4bit=True,\n)\nbase_model = FastLanguageModel.for_inference(base_model)\n\n# Load LoRA adapter on top of the base model\nft_model = PeftModel.from_pretrained(base_model, ADAPTER_DIR)\nft_model = FastLanguageModel.for_inference(ft_model)\n\n# Generate answers and build comparison table\nresults = []\nfor i, r in enumerate(rows, 1):\n    sys, usr, ref = extract_sys_user_ref(r.get(\"messages\", []))\n    print(f\"â³ Evaluating question {i}/{len(rows)}...\")\n    base_ans = generate_answer(base_model, base_tokenizer, sys, usr)\n    ft_ans = generate_answer(ft_model, base_tokenizer, sys, usr)\n    results.append({\n        \"question\": usr,\n        \"reference\": ref,\n        \"base\": base_ans,\n        \"finetuned\": ft_ans,\n    })\n\n# Display as a nicely formatted HTML table (RTL for Hebrew)\nhtml = \"\"\"\n<style>\n  .eval-table { border-collapse: collapse; width: 100%; direction: rtl; text-align: right; font-size: 13px; }\n  .eval-table th { background: #1a1a2e; color: #e0e0e0; padding: 10px; border: 1px solid #333; }\n  .eval-table td { padding: 10px; border: 1px solid #ddd; vertical-align: top; }\n  .eval-table tr:nth-child(even) { background: #f8f8f8; }\n  .eval-table .q { font-weight: bold; color: #1a1a2e; }\n  .eval-table .ref { color: #2d6a4f; }\n  .eval-table .base { color: #9d0208; }\n  .eval-table .ft { color: #023e8a; }\n</style>\n<h3 style=\"direction:rtl; text-align:right;\">ğŸ“Š ×”×©×•×•××”: ××•×“×œ ×‘×¡×™×¡ ××•×œ ××•×“×œ ×××•××Ÿ</h3>\n<table class=\"eval-table\">\n  <tr>\n    <th>#</th>\n    <th>×©××œ×”</th>\n    <th>×ª×©×•×‘×ª ×”×™×™×—×•×¡</th>\n    <th>××•×“×œ ×‘×¡×™×¡</th>\n    <th>××•×“×œ ×××•××Ÿ (LoRA)</th>\n  </tr>\n\"\"\"\n\nfor i, r in enumerate(results, 1):\n    html += f\"\"\"\n  <tr>\n    <td>{i}</td>\n    <td class=\"q\">{r['question']}</td>\n    <td class=\"ref\">{r['reference'][:300]}</td>\n    <td class=\"base\">{r['base'][:300]}</td>\n    <td class=\"ft\">{r['finetuned'][:300]}</td>\n  </tr>\n\"\"\"\n\nhtml += \"</table>\"\ndisplay(HTML(html))\n\nprint(\"\\nâœ… Evaluation complete.\")",
   "execution_count": null,
   "outputs": [],
   "id": "11c09124"
  },
  {
   "cell_type": "markdown",
   "id": "7qpstlmmmzx",
   "source": "## Part E â€” Interactive Web Interface (Gradio)\n\nLaunch a **Hebrew-RTL chat interface** to interact with the fine-tuned model.\n\n- Runs directly in Colab (or locally with a GPU).\n- Provides a shareable public link via Gradio's `share=True`.\n- Styled with a dark theme and RTL support for Hebrew text.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "6q1tuorcxk",
   "source": "%pip install gradio -q\n\nimport gradio as gr\nimport torch\nimport os\nfrom peft import PeftModel\nfrom unsloth import FastLanguageModel\n\n# --- Load model (reuses if already in memory) ---\nBASE_MODEL_NAME = \"unsloth/gemma-2-2b-it-bnb-4bit\"\nADAPTER_DIR = \"outputs/lora_adapter\"\n\nif \"ft_model\" not in dir() or ft_model is None:\n    print(\"â³ Loading model...\")\n    base_model, base_tokenizer = FastLanguageModel.from_pretrained(\n        model_name=BASE_MODEL_NAME, max_seq_length=2048, dtype=None, load_in_4bit=True,\n    )\n    ft_model = PeftModel.from_pretrained(base_model, ADAPTER_DIR)\n    ft_model = FastLanguageModel.for_inference(ft_model)\n    tokenizer = base_tokenizer\n    print(\"âœ… Model loaded!\")\n\nSYSTEM_PROMPT = \"××ª×” ××•×¨×” ×¢×–×¨ ×œ××–×¨×—×•×ª ×©××›×™×Ÿ ×ª×œ××™×“×™× ×œ×‘×’×¨×•×ª ×‘×™×©×¨××œ.\"\n\ndef chat_fn(message, history):\n    user_input = f\"{SYSTEM_PROMPT}\\n\\n{message}\"\n    prompt = tokenizer.apply_chat_template(\n        [{\"role\": \"user\", \"content\": user_input}],\n        tokenize=False,\n        add_generation_prompt=True,\n    )\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n    with torch.no_grad():\n        out = ft_model.generate(\n            **inputs,\n            max_new_tokens=300,\n            do_sample=False,\n        )\n    answer_tokens = out[0][inputs[\"input_ids\"].shape[-1]:]\n    return tokenizer.decode(answer_tokens, skip_special_tokens=True).strip()\n\n# --- Custom CSS for Hebrew RTL + polished look ---\ncustom_css = \"\"\"\n/* RTL for the whole app */\n.gradio-container { direction: rtl !important; }\n\n/* Chat bubbles */\n.message { direction: rtl !important; text-align: right !important; font-size: 16px !important; line-height: 1.8 !important; }\n\n/* Input box */\ntextarea { direction: rtl !important; text-align: right !important; font-size: 16px !important; }\n\n/* Header */\n.gr-header { text-align: center !important; }\n\n/* Nice gradient header */\n#component-0 { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%) !important;\n               border-radius: 12px !important; padding: 20px !important; margin-bottom: 16px !important; }\n\"\"\"\n\n# --- Build the interface ---\nwith gr.Blocks(\n    theme=gr.themes.Soft(primary_hue=\"indigo\", secondary_hue=\"purple\"),\n    css=custom_css,\n    title=\"BagrutAI â€” ××•×¨×” ×¢×–×¨ ×œ××–×¨×—×•×ª\",\n) as demo:\n\n    gr.HTML(\"\"\"\n    <div style=\"text-align:center; padding: 24px 16px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n                border-radius: 16px; margin-bottom: 20px; color: white;\">\n        <h1 style=\"margin:0; font-size:2.2em;\">ğŸ“ BagrutAI</h1>\n        <p style=\"margin:8px 0 0 0; font-size:1.1em; opacity:0.9;\">××•×¨×” ×¢×–×¨ ×—×›× ×œ××–×¨×—×•×ª â€” ×”×›× ×” ×œ×‘×’×¨×•×ª ×‘×™×©×¨××œ</p>\n    </div>\n    \"\"\")\n\n    chatbot = gr.ChatInterface(\n        fn=chat_fn,\n        chatbot=gr.Chatbot(\n            height=480,\n            placeholder=\"<div style='text-align:center; color:#888; padding:40px;'>×©××œ ××•×ª×™ ×©××œ×” ××‘×—×™× ×ª ×”×‘×’×¨×•×ª ×‘××–×¨×—×•×ª ğŸ“š</div>\",\n            rtl=True,\n        ),\n        textbox=gr.Textbox(\n            placeholder=\"...×›×ª×•×‘ ×©××œ×” ×‘××–×¨×—×•×ª\",\n            rtl=True,\n            scale=7,\n        ),\n        submit_btn=\"×©×œ×—\",\n        retry_btn=\"× ×¡×” ×©×•×‘\",\n        undo_btn=\"×‘×˜×œ\",\n        clear_btn=\"× ×§×” ×”×›×œ\",\n        examples=[\n            \"×”×¦×™×’×• ××ª ×¢×™×§×¨×™ ×—×•×§ ×”×©×‘×•×ª.\",\n            \"×”×¦×™×’×• ××ª ×”××•×©×’ ×‘×™×§×•×¨×ª ×©×™×¤×•×˜×™×ª.\",\n            \"×”×¦×™×’×• ××ª ×”××•×©×’ ×“××•×§×¨×˜×™×” ××ª×’×•× × ×ª.\",\n            \"×”×¦×™×’×• ××ª ×¢×™×§×¨×™ ×”×¡×“×¨ ×”×¡×˜×˜×•×¡Ö¾×§×•×•.\",\n            \"××”× ×”×ª× ××™× ×”×”×›×¨×—×™×™× ×œ×§×™×•× ×‘×—×™×¨×•×ª ×“××•×§×¨×˜×™×•×ª?\",\n        ],\n        cache_examples=False,\n    )\n\n    gr.HTML(\"\"\"\n    <div style=\"text-align:center; padding:12px; color:#888; font-size:0.85em; border-top:1px solid #eee; margin-top:16px;\">\n        BagrutAI â€” ×¤×¨×•×™×§×˜ ×’××¨ ×‘×™×ª ×¡×¤×¨×™ | ××‘×•×¡×¡ ×¢×œ Gemma 2 2B + LoRA<br>\n        âš ï¸ ×”××•×“×œ ×¢×œ×•×œ ×œ×˜×¢×•×ª â€” ×™×© ×œ×”×©×ª××© ×›×›×œ×™ ×¢×–×¨ ×œ×œ××™×“×” ×‘×œ×‘×“\n    </div>\n    \"\"\")\n\ndemo.launch(share=True, debug=False)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}