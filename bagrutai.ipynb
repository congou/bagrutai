{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BagrutAI â€” Fineâ€‘tuning an Israeli Civics (××–×¨×—×•×ª) â€œBagrut tutorâ€\n",
        "\n",
        "**Final high school project**\n",
        "\n",
        "- **Student**: *(fill in)*\n",
        "- **School / Class**: *(fill in)*\n",
        "- **Date**: *(fill in)*\n",
        "\n",
        "- **Goal**: Fineâ€‘tune a small language model to answer Israeli Civics (××–×¨×—×•×ª) Bagrutâ€‘style questions in **Hebrew**, with **short, factual** answers.\n",
        "- **Approach**: Combine **real** Bagrut Q/A with **synthetic** Q/A generated from the study material, then fineâ€‘tune using **LoRA**.\n",
        "\n",
        "## What this notebook does\n",
        "\n",
        "- **Part A â€” Synthetic data generation**: Generate additional Bagrutâ€‘style questions and answers from a civics study document (`civics.docx`).\n",
        "- **Part B â€” Dataset build**: Clean, normalize, and combine real + synthetic data into a single training dataset.\n",
        "- **Part C â€” Fineâ€‘tuning**: Fineâ€‘tune `Gemma 2 2B` (4â€‘bit) with LoRA using Unsloth.\n",
        "- **Part D â€” Evaluation**: Evaluate qualitatively on a **realâ€‘only** holdout set.\n",
        "\n",
        "## How to run (recommended order)\n",
        "\n",
        "1. *(Optional)* Run **Part A** if you want to regenerate synthetic data.\n",
        "2. Run **Part B/C** to build `master_training_data.jsonl` and fineâ€‘tune the model.\n",
        "3. Run **Part D** to compare **base vs fineâ€‘tuned** answers on `test_real.jsonl`.\n",
        "\n",
        "## Project files (JSONL)\n",
        "\n",
        "- `training_data.jsonl`: **Real** Bagrut questions & answers.\n",
        "- `synthetic_data_massive.jsonl`: **Synthetic** Q/A generated from the study document.\n",
        "- `master_training_data.jsonl`: The **reproducible training mix** written by this notebook.\n",
        "- `test_real.jsonl`: A **realâ€‘only holdout set** for final evaluation (never used in training).\n",
        "\n",
        "## Reproducibility\n",
        "\n",
        "- A fixed seed is used (`SEED = 3407`) so sampling and splits are repeatable.\n",
        "- The notebook writes out the exact training mix so results can be reproduced.\n",
        "\n",
        "## Responsible use\n",
        "\n",
        "- This model may still **make mistakes** or **hallucinate**; it should be used as a study aid.\n",
        "- Real exam questions may be copyrighted; avoid redistributing proprietary materials.\n",
        "\n",
        "## References (tools used)\n",
        "\n",
        "- Unsloth (LoRA fineâ€‘tuning)\n",
        "- Hugging Face `datasets`\n",
        "- TRL `SFTTrainer`\n",
        "\n",
        "> Tip for your report: include screenshots of dataset sizes, training/eval loss, and before/after answers on `test_real.jsonl`.\n",
        ""
      ],
      "id": "be2ead32"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## Part A â€” Generate synthetic Bagrutâ€‘style Q/A from study material\n",
        "\n",
        "This section generates **additional practice questions** from a civics study document.\n",
        "\n",
        "### Inputs\n",
        "\n",
        "- **`civics.docx`**: a Word document containing the civics study material.\n",
        "- **Gemini API key**: stored in Colab Secrets as `GEMINI_API_KEY`.\n",
        "\n",
        "### Output\n",
        "\n",
        "- **`synthetic_data_massive.jsonl`**: one JSON object per line, formatted as:\n",
        "\n",
        "```json\n",
        "{\"messages\": [{\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"model\", \"content\": \"...\"}]}\n",
        "```\n",
        "\n",
        "### Quality guidelines (important)\n",
        "\n",
        "- Synthetic data should be **short, factual, and examâ€‘style**.\n",
        "- If the model returns invalid JSON, the code skips that chunk.\n",
        "- After generating synthetic data, we later **mix it with real data** (and keep a realâ€‘only test set) to reduce â€œAIâ€‘styleâ€ artifacts.\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "a95afd83"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1. Install libraries\n",
        "%pip install google-genai python-docx -q\n",
        "\n",
        "from google import genai\n",
        "import docx\n",
        "import json\n",
        "import time\n",
        "from google.colab import userdata\n",
        "\n",
        "# 2. Connect to the API\n",
        "try:\n",
        "    GOOGLE_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "    print(\"âœ… Successfully connected to the API!\")\n",
        "except Exception as e:\n",
        "    print(\"âŒ Connection error. Make sure the key is defined in Secrets.\")\n",
        "    raise e\n",
        "\n",
        "# 3. Read the Word file and divide into chunks\n",
        "word_file_path = \"/content/civics.docx\" # Change this if your file name is different!\n",
        "\n",
        "print(f\"ğŸ“„ Reading the file {word_file_path}...\")\n",
        "try:\n",
        "    doc = docx.Document(word_file_path)\n",
        "except Exception as e:\n",
        "    raise FileNotFoundError(f\"Could not find the file '{word_file_path}'. Did you upload it to the sidebar?\")\n",
        "\n",
        "# Collect all text and divide into chunks\n",
        "paragraphs = [para.text for para in doc.paragraphs if para.text.strip()]\n",
        "full_text = \"\\n\".join(paragraphs)\n",
        "\n",
        "chunk_size = 4000 # Number of characters per chunk (about 1.5 to 2 pages)\n",
        "text_chunks = [full_text[i:i + chunk_size] for i in range(0, len(full_text), chunk_size)]\n",
        "\n",
        "print(f\"ğŸ“š The file was divided into {len(text_chunks)} chunks. Starting question generation!\")\n",
        "\n",
        "system_instruction = \"××ª×” ××•×¨×” ×¢×–×¨ ×œ××–×¨×—×•×ª ×©××›×™×Ÿ ×ª×œ××™×“×™× ×œ×‘×’×¨×•×ª ×‘×™×©×¨××œ.\"\n",
        "total_generated = 0\n",
        "\n",
        "# 4. Loop through each chunk and generate questions\n",
        "for index, chunk in enumerate(text_chunks):\n",
        "    print(f\"â³ Processing chunk {index + 1} out of {len(text_chunks)}...\")\n",
        "    \n",
        "    prompt = f\"\"\"\n",
        "    You are an expert Israeli Civics teacher creating training data for an AI.\n",
        "    Read the following text and generate up to 10 official Bagrut-style questions and answers based ONLY on this text.\n",
        "    \n",
        "    The answers must be short, factual, and written in Hebrew.\n",
        "    \n",
        "    Format your output STRICTLY as a JSON list of objects, like this:\n",
        "    [\n",
        "      {{\"question\": \"...\", \"answer\": \"...\"}}\n",
        "    ]\n",
        "    \n",
        "    Text to use:\n",
        "    {chunk}\n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        # Call Gemini 2.5 Flash\n",
        "        response = client.models.generate_content(\n",
        "            model='gemini-2.5-flash',\n",
        "            contents=prompt\n",
        "        )\n",
        "        \n",
        "        # Clean and parse the JSON\n",
        "        json_text = response.text.strip().replace(\"```json\", \"\").replace(\"```\", \"\")\n",
        "        new_qa_pairs = json.loads(json_text)\n",
        "        \n",
        "        # Save the questions to a new file\n",
        "        with open(\"synthetic_data_massive.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
        "            for pair in new_qa_pairs:\n",
        "                user_input = f\"{system_instruction}\\n\\n{pair['question']}\"\n",
        "                entry = {\n",
        "                    \"messages\": [\n",
        "                        {\"role\": \"user\", \"content\": user_input},\n",
        "                        {\"role\": \"model\", \"content\": pair['answer']}\n",
        "                    ]\n",
        "                }\n",
        "                f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
        "        \n",
        "        total_generated += len(new_qa_pairs)\n",
        "        print(f\"âœ… Added {len(new_qa_pairs)} questions (Total so far: {total_generated}).\")\n",
        "        \n",
        "        # A short 3-second pause to respect the API quota (Rate Limit)\n",
        "        time.sleep(3)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Error in chunk {index + 1}, the model probably returned an invalid format. Skipping to the next chunk.\")\n",
        "\n",
        "print(f\"ğŸ‰ We are done! The file 'synthetic_data_massive.jsonl' is ready with {total_generated} questions and answers.\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "62e3367c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part B/C â€” Build dataset + Fineâ€‘tune with LoRA (Unsloth)\n",
        "\n",
        "This section:\n",
        "\n",
        "1. Loads **real** (`training_data.jsonl`) and **synthetic** (`synthetic_data_massive.jsonl`) datasets.\n",
        "2. Cleans and normalizes message format (including fixing synthetic rows where the â€œsystemâ€ prompt was embedded inside the user message).\n",
        "3. Creates a **realâ€‘only holdout test set** (`test_real.jsonl`).\n",
        "4. Builds a controlled **train mix** (default: **80% real / 20% synthetic**) and saves it as `master_training_data.jsonl`.\n",
        "5. Fineâ€‘tunes a small instruction model using **4â€‘bit + LoRA**, which is practical on Google Colab GPUs.\n",
        "\n",
        "### Why Gemma 2 2B + LoRA?\n",
        "\n",
        "- **Small enough** to train on Colab.\n",
        "- **LoRA** updates only a small number of parameters, making training fast and memoryâ€‘efficient.\n",
        "- **4â€‘bit** quantization reduces VRAM usage while keeping good quality for an educational demo.\n",
        "\n",
        "### Key outputs\n",
        "\n",
        "- `outputs/lora_adapter/`: the trained LoRA adapter (can be reloaded later).\n",
        "- `test_real.jsonl`: the realâ€‘only holdout set used for final evaluation.\n"
      ],
      "id": "d68e4772"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1. INSTALL & SETUP\n",
        "try:\n",
        "    import unsloth\n",
        "except ImportError:\n",
        "    print(\"â³ Installing Unsloth... (approx 2 mins)\")\n",
        "    %pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "    %pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import Dataset, load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "\n",
        "# 2. LOAD MODEL (Switched to 2B for stability)\n",
        "# The 2B model is lighter and runs perfectly on Colab's standard GPU\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "print(\"â³ Loading Gemma 2 (2B) model...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-2-2b-it-bnb-4bit\",  # <--- CHANGED TO 2B\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "# 3. PREPARE DATA\n",
        "print(\"â³ Preparing dataset...\")\n",
        "\n",
        "REAL_FILE = \"training_data.jsonl\"\n",
        "SYNTH_FILE = \"synthetic_data_massive.jsonl\"\n",
        "MASTER_FILE = \"master_training_data.jsonl\"  # optional fallback\n",
        "\n",
        "SYNTH_RATIO = 0.20           # target fraction of synthetic in training mix\n",
        "REAL_TEST_FRACTION = 0.10    # real-only holdout fraction\n",
        "SEED = 3407\n",
        "\n",
        "teacher_prefix = \"××ª×” ××•×¨×” ×¢×–×¨ ×œ××–×¨×—×•×ª\"\n",
        "\n",
        "\n",
        "def read_jsonl(path):\n",
        "    rows = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            rows.append(json.loads(line))\n",
        "    return rows\n",
        "\n",
        "\n",
        "def normalize_messages(messages):\n",
        "    if not isinstance(messages, list):\n",
        "        return None\n",
        "\n",
        "    sys = next((m.get(\"content\") for m in messages if isinstance(m, dict) and m.get(\"role\") == \"system\"), \"\")\n",
        "    usr = next((m.get(\"content\") for m in messages if isinstance(m, dict) and m.get(\"role\") == \"user\"), \"\")\n",
        "    mod = next(\n",
        "        (m.get(\"content\") for m in messages if isinstance(m, dict) and m.get(\"role\") in {\"model\", \"assistant\"}),\n",
        "        \"\",\n",
        "    )\n",
        "\n",
        "    # Synthetic file often embedded the system instruction in the user message.\n",
        "    if (not sys) and isinstance(usr, str):\n",
        "        u = usr.strip()\n",
        "        if u.startswith(teacher_prefix) and \"\\n\\n\" in u:\n",
        "            sys_candidate, rest = u.split(\"\\n\\n\", 1)\n",
        "            if sys_candidate.strip().startswith(teacher_prefix):\n",
        "                sys = sys_candidate.strip()\n",
        "                usr = rest.strip()\n",
        "\n",
        "    if not isinstance(usr, str) or not usr.strip() or not isinstance(mod, str) or not mod.strip():\n",
        "        return None\n",
        "\n",
        "    norm = []\n",
        "    if isinstance(sys, str) and sys.strip():\n",
        "        norm.append({\"role\": \"system\", \"content\": sys.strip()})\n",
        "    norm.append({\"role\": \"user\", \"content\": usr.strip()})\n",
        "    norm.append({\"role\": \"model\", \"content\": mod.strip()})\n",
        "    return norm\n",
        "\n",
        "\n",
        "def build_dataset_from_sources():\n",
        "    has_real = os.path.exists(REAL_FILE)\n",
        "    has_synth = os.path.exists(SYNTH_FILE)\n",
        "\n",
        "    if has_real and has_synth:\n",
        "        real_rows = read_jsonl(REAL_FILE)\n",
        "        synth_rows = read_jsonl(SYNTH_FILE)\n",
        "\n",
        "        real = []\n",
        "        synth = []\n",
        "\n",
        "        for r in real_rows:\n",
        "            msgs = normalize_messages(r.get(\"messages\"))\n",
        "            if msgs is not None:\n",
        "                real.append({\"messages\": msgs, \"source\": \"real\"})\n",
        "\n",
        "        for r in synth_rows:\n",
        "            msgs = normalize_messages(r.get(\"messages\"))\n",
        "            if msgs is not None:\n",
        "                synth.append({\"messages\": msgs, \"source\": \"synthetic\"})\n",
        "\n",
        "        if not real:\n",
        "            raise ValueError(\"Real dataset loaded but produced 0 usable rows after cleaning.\")\n",
        "\n",
        "        rng = random.Random(SEED)\n",
        "\n",
        "        # Real-only test set\n",
        "        n_test = max(10, int(len(real) * REAL_TEST_FRACTION))\n",
        "        n_test = min(n_test, max(10, len(real) - 1))\n",
        "        test_real = rng.sample(real, n_test)\n",
        "        test_ids = {id(x) for x in test_real}\n",
        "        real_train = [x for x in real if id(x) not in test_ids]\n",
        "\n",
        "        # Keep all real_train; downsample synthetic to desired ratio\n",
        "        if synth and 0.0 < SYNTH_RATIO < 1.0:\n",
        "            target_synth = int(len(real_train) * (SYNTH_RATIO / (1.0 - SYNTH_RATIO)))\n",
        "            target_synth = max(0, min(target_synth, len(synth)))\n",
        "            synth_selected = rng.sample(synth, target_synth) if target_synth else []\n",
        "        else:\n",
        "            synth_selected = []\n",
        "\n",
        "        combined = real_train + synth_selected\n",
        "        rng.shuffle(combined)\n",
        "\n",
        "        # Save the real-only test set for final evaluation/demo\n",
        "        with open(\"test_real.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "            for row in test_real:\n",
        "                f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "        # Save combined training set for reproducibility\n",
        "        with open(MASTER_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "            for row in combined:\n",
        "                f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "        print(\n",
        "            \"âœ… Built combined dataset from sources:\\n\"\n",
        "            f\"- real usable: {len(real)}\\n\"\n",
        "            f\"- synthetic usable: {len(synth)}\\n\"\n",
        "            f\"- train mix (real): {len(real_train)}\\n\"\n",
        "            f\"- train mix (synthetic): {len(synth_selected)}\\n\"\n",
        "            f\"- saved: {MASTER_FILE} (train mix)\\n\"\n",
        "            f\"- saved: test_real.jsonl (real-only holdout)\"\n",
        "        )\n",
        "\n",
        "        return Dataset.from_list(combined)\n",
        "\n",
        "    # Fallback to an already-combined file\n",
        "    for p in [MASTER_FILE, REAL_FILE]:\n",
        "        if os.path.exists(p):\n",
        "            print(f\"âœ… Using existing dataset file: {p}\")\n",
        "            return load_dataset(\"json\", data_files=p, split=\"train\")\n",
        "\n",
        "    raise FileNotFoundError(\n",
        "        f\"Could not find '{REAL_FILE}' and '{SYNTH_FILE}', and no fallback '{MASTER_FILE}'.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def format_chat_template_fixed(examples):\n",
        "    texts = []\n",
        "    for messages in examples[\"messages\"]:\n",
        "        if not isinstance(messages, list):\n",
        "            texts.append(\"\")\n",
        "            continue\n",
        "\n",
        "        sys = next((m.get(\"content\") for m in messages if isinstance(m, dict) and m.get(\"role\") == \"system\"), \"\")\n",
        "        usr = next((m.get(\"content\") for m in messages if isinstance(m, dict) and m.get(\"role\") == \"user\"), \"\")\n",
        "        mod = next((m.get(\"content\") for m in messages if isinstance(m, dict) and m.get(\"role\") == \"model\"), \"\")\n",
        "\n",
        "        user_input = f\"{sys}\\n\\n{usr}\" if sys else usr\n",
        "        if not isinstance(user_input, str) or not user_input.strip() or not isinstance(mod, str) or not mod.strip():\n",
        "            texts.append(\"\")\n",
        "            continue\n",
        "\n",
        "        conversation = tokenizer.apply_chat_template(\n",
        "            [\n",
        "                {\"role\": \"user\", \"content\": user_input.strip()},\n",
        "                {\"role\": \"model\", \"content\": mod.strip()},\n",
        "            ],\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False,\n",
        "        )\n",
        "        texts.append(conversation)\n",
        "\n",
        "    return {\"text\": texts}\n",
        "\n",
        "raw_dataset = build_dataset_from_sources()\n",
        "dataset = raw_dataset.map(format_chat_template_fixed, batched=True)\n",
        "dataset = dataset.filter(lambda x: bool(x.get(\"text\")))\n",
        "\n",
        "split_ds = dataset.train_test_split(test_size=0.05, seed=SEED)\n",
        "train_dataset = split_ds[\"train\"]\n",
        "eval_dataset = split_ds[\"test\"]\n",
        "\n",
        "# 4. SETUP LORA\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        ")\n",
        "\n",
        "# 5. TRAIN\n",
        "print(\"ğŸš€ Starting Training... (This will be fast!)\")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = eval_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        evaluation_strategy = \"steps\",\n",
        "        eval_steps = 10,\n",
        "        save_strategy = \"steps\",\n",
        "        save_steps = 10,\n",
        "        save_total_limit = 2,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "trainer.model.save_pretrained(\"outputs/lora_adapter\")\n",
        "tokenizer.save_pretrained(\"outputs/lora_adapter\")\n",
        "\n",
        "print(\"ğŸ‰ Training Complete! Saved adapter to outputs/lora_adapter\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "76b99d78"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part D â€” Evaluation (realâ€‘only holdout)\n",
        "\n",
        "To make this a strong final project, we evaluate on a **realâ€‘only** set (`test_real.jsonl`) that was **never used for training**.\n",
        "\n",
        "### What to look for\n",
        "\n",
        "- **Correctness**: does the answer match the civics material?\n",
        "- **Bagrut style**: short, factual, uses proper terms (e.g., ×¢×§×¨×•× ×•×ª ×“××•×§×¨×˜×™×™×, ×—×•×§×™ ×™×¡×•×“).\n",
        "- **No hallucinations**: if unsure, the answer should be conservative.\n",
        "\n",
        "### Suggested writeâ€‘up structure (easy to grade)\n",
        "\n",
        "- **Data**: where each dataset came from (real vs synthetic), and why mixing helps.\n",
        "- **Method**: LoRA fineâ€‘tuning (why itâ€™s efficient), model choice, key hyperparameters.\n",
        "- **Evaluation**: realâ€‘only holdout + before/after examples.\n",
        "- **Limitations**: dataset size, synthetic artifacts, no guarantee of correctness.\n",
        "- **Future work**: more real questions, dedup, more question types (×§×˜×¢/××™×¨×•×¢), better refusal behavior.\n"
      ],
      "id": "aa6d4e8e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Demo: compare base vs fine-tuned on real-only holdout\n",
        "# Run this AFTER training (requires outputs/lora_adapter and test_real.jsonl).\n",
        "\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "\n",
        "from peft import PeftModel\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "BASE_MODEL_NAME = \"unsloth/gemma-2-2b-it-bnb-4bit\"\n",
        "ADAPTER_DIR = \"outputs/lora_adapter\"\n",
        "HOLDOUT_FILE = \"test_real.jsonl\"\n",
        "\n",
        "N_SAMPLES = 10\n",
        "MAX_NEW_TOKENS = 200\n",
        "SEED = 3407\n",
        "\n",
        "\n",
        "def load_holdout(path):\n",
        "    rows = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            rows.append(json.loads(line))\n",
        "    return rows\n",
        "\n",
        "\n",
        "def extract_sys_user(messages):\n",
        "    sys = \"\"\n",
        "    usr = \"\"\n",
        "    for m in messages:\n",
        "        if not isinstance(m, dict):\n",
        "            continue\n",
        "        if m.get(\"role\") == \"system\" and not sys:\n",
        "            sys = m.get(\"content\", \"\")\n",
        "        if m.get(\"role\") == \"user\" and not usr:\n",
        "            usr = m.get(\"content\", \"\")\n",
        "    return (sys or \"\").strip(), (usr or \"\").strip()\n",
        "\n",
        "\n",
        "def generate_answer(model, tokenizer, sys, usr):\n",
        "    user_input = f\"{sys}\\n\\n{usr}\" if sys else usr\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        [{\"role\": \"user\", \"content\": user_input}],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "    )\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "            do_sample=False,\n",
        "        )\n",
        "\n",
        "    answer_tokens = out[0][inputs[\"input_ids\"].shape[-1] :]\n",
        "    return tokenizer.decode(answer_tokens, skip_special_tokens=True).strip()\n",
        "\n",
        "\n",
        "# Load holdout questions\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"âš ï¸ GPU not detected. This will be slow on CPU.\")\n",
        "\n",
        "if not (os.path.exists(HOLDOUT_FILE) and os.path.exists(ADAPTER_DIR)):\n",
        "    raise FileNotFoundError(\n",
        "        f\"Missing '{HOLDOUT_FILE}' or '{ADAPTER_DIR}'. Run the training cell first.\"\n",
        "    )\n",
        "\n",
        "rows = load_holdout(HOLDOUT_FILE)\n",
        "rng = random.Random(SEED)\n",
        "rng.shuffle(rows)\n",
        "rows = rows[: min(N_SAMPLES, len(rows))]\n",
        "\n",
        "# Load base model\n",
        "base_model, base_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=BASE_MODEL_NAME,\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "base_model = FastLanguageModel.for_inference(base_model)\n",
        "\n",
        "print(\"\\n===== BASE MODEL (before fine-tuning) =====\\n\")\n",
        "base_answers = []\n",
        "for i, r in enumerate(rows, 1):\n",
        "    sys, usr = extract_sys_user(r.get(\"messages\", []))\n",
        "    ans = generate_answer(base_model, base_tokenizer, sys, usr)\n",
        "    base_answers.append(ans)\n",
        "    print(f\"[{i}] ×©××œ×”: {usr}\\n×ª×©×•×‘×” (×‘×¡×™×¡): {ans}\\n\")\n",
        "\n",
        "# Load LoRA adapter on top of the base model\n",
        "ft_model = PeftModel.from_pretrained(base_model, ADAPTER_DIR)\n",
        "ft_model = FastLanguageModel.for_inference(ft_model)\n",
        "\n",
        "print(\"\\n===== FINE-TUNED MODEL (LoRA adapter) =====\\n\")\n",
        "for i, r in enumerate(rows, 1):\n",
        "    sys, usr = extract_sys_user(r.get(\"messages\", []))\n",
        "    ans = generate_answer(ft_model, base_tokenizer, sys, usr)\n",
        "    print(f\"[{i}] ×©××œ×”: {usr}\\n×ª×©×•×‘×” (×××•××Ÿ): {ans}\\n\")\n",
        "\n",
        "print(\"Done.\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "11c09124"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}