# BagrutAI â€” ××•×¨×” ×¢×–×¨ ×—×›× ×œ××–×¨×—×•×ª ğŸ“

An AI-powered study assistant for the Israeli Civics (××–×¨×—×•×ª) Bagrut exam, fine-tuned to answer exam-style questions in Hebrew with short, factual responses.

> **Final high-school project** â€” Fine-tuning a small language model on real and synthetic Bagrut Q&A data using LoRA.

---

## Overview

BagrutAI fine-tunes **Gemma 2 2B** (4-bit quantized) to act as a civics tutor that answers Israeli Bagrut-style questions. The project combines:

- **Real exam Q&A** â€” Verified questions and answers from past Bagrut exams
- **Synthetic data** â€” AI-generated questions from civics study material (via Google Gemini)
- **LoRA fine-tuning** â€” Parameter-efficient training that runs on a free Google Colab GPU

The result is a model that produces concise, exam-appropriate Hebrew answers covering topics like democratic principles, government structure, elections, minority rights, and religion-state relations.

## Architecture

```
civics.docx (study material)
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Gemini 2.5 Flash   â”‚  â† Generates synthetic Q&A
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â–¼
  synthetic_data.jsonl
          â”‚
          â”œâ”€â”€ training_data.jsonl (real exam Q&A)
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Normalize + Dedup   â”‚
â”‚  Split + Mix (80/20) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Gemma 2 2B (4-bit) â”‚
â”‚  + LoRA (r=32)       â”‚
â”‚  Fine-tune w/ SFT    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â–¼
   LoRA Adapter â”€â”€â–º Gradio Chat UI
```

## Quick Start

### 1. Open in Colab

Upload `bagrutai.ipynb` to [Google Colab](https://colab.research.google.com/) (use a **T4 GPU** runtime).

### 2. Upload data files

Upload these files to the Colab file browser:
- `training_data.jsonl` â€” Real Bagrut Q&A
- `synthetic_data_massive.jsonl` â€” Synthetic Q&A
- `civics.docx` â€” *(only if regenerating synthetic data)*

### 3. Run the notebook

| Part | What it does |
|------|-------------|
| **A** | *(Optional)* Generates synthetic Q&A from `civics.docx` using Gemini API |
| **B/C** | Builds the training dataset, then fine-tunes the model with LoRA |
| **D** | Evaluates base vs. fine-tuned model on a real-only holdout set |
| **E** | Launches an interactive Gradio chat interface |

### 4. API key (for Part A only)

Store your Google Gemini API key in Colab Secrets as `GEMINI_API_KEY`.

## Project Files

| File | Description |
|------|-------------|
| `bagrutai.ipynb` | Main notebook â€” full pipeline from data to UI |
| `training_data.jsonl` | ~60 real Bagrut civics Q&A pairs |
| `synthetic_data_massive.jsonl` | AI-generated Q&A from study material |
| `master_training_data.jsonl` | Reproducible training mix (generated by notebook) |
| `test_real.jsonl` | Real-only holdout set for evaluation (generated by notebook) |

## Training Details

| Parameter | Value |
|-----------|-------|
| Base model | Gemma 2 2B (`unsloth/gemma-2-2b-it-bnb-4bit`) |
| Quantization | 4-bit (bitsandbytes) |
| Method | LoRA via Unsloth |
| LoRA rank (r) | 32 |
| LoRA alpha | 64 |
| Target modules | q, k, v, o, gate, up, down projections |
| Optimizer | AdamW 8-bit |
| LR scheduler | Cosine decay |
| Learning rate | 2e-4 |
| Training steps | 120 |
| Batch size | 2 (+ 4 gradient accumulation) |
| Sequence length | 2048 tokens |
| Train/synth ratio | 80% real / 20% synthetic |
| Seed | 3407 |

## Data Format

Each entry in the JSONL files follows this format:

```json
{
  "messages": [
    {"role": "system", "content": "××ª×” ××•×¨×” ×¢×–×¨ ×œ××–×¨×—×•×ª ×©××›×™×Ÿ ×ª×œ××™×“×™× ×œ×‘×’×¨×•×ª ×‘×™×©×¨××œ."},
    {"role": "user", "content": "×”×¦×™×’×• ××ª ×¢×™×§×¨×™ ×—×•×§ ×”×©×‘×•×ª."},
    {"role": "model", "content": "×—×•×§ ××©× ×ª 1950 ×”×§×•×‘×¢ ×©×›×œ ×™×”×•×“×™ ×–×›××™ ×œ×¢×œ×•×ª ×œ××¨×¥..."}
  ]
}
```

## Topics Covered

- Constitutional law (×—×•×§×™ ×™×¡×•×“)
- Declaration of Independence (×”×›×¨×–×ª ×”×¢×¦×××•×ª)
- Democratic principles (×©×œ×˜×•×Ÿ ×”×¢×, ×”×¡×›××™×•×ª, ×¤×œ×•×¨×œ×™×–×, ×¡×•×‘×œ× ×•×ª)
- Government structure & parliamentary system (×××©×œ ×¤×¨×œ×× ×˜×¨×™)
- Elections and coalitions (×‘×—×™×¨×•×ª, ×§×•××œ×™×¦×™×”)
- Judicial review (×‘×™×§×•×¨×ª ×©×™×¤×•×˜×™×ª)
- Religion and state (×“×ª ×•××“×™× ×”, ×”×¡×˜×˜×•×¡ ×§×•×•)
- Minority rights (×–×›×•×™×•×ª ××™×¢×•×˜)
- Media and civic participation (×ª×§×©×•×¨×ª, ×§×‘×•×¦×•×ª ××™× ×˜×¨×¡)
- Local government (×©×œ×˜×•×Ÿ ××§×•××™)

## Limitations

- **Small dataset** â€” ~60 real questions; the model may not generalize to all question types
- **May hallucinate** â€” Answers should be verified against study material
- **Hebrew-only** â€” Trained specifically for Hebrew civics content
- **Study aid only** â€” Not a replacement for actual exam preparation with a teacher

## Tech Stack

- [Unsloth](https://github.com/unslothai/unsloth) â€” Fast LoRA fine-tuning
- [Hugging Face Transformers](https://huggingface.co/docs/transformers) â€” Model loading & tokenization
- [TRL](https://huggingface.co/docs/trl) â€” SFTTrainer for supervised fine-tuning
- [Gradio](https://gradio.app/) â€” Interactive chat interface
- [Google Gemini API](https://ai.google.dev/) â€” Synthetic data generation
- [Google Colab](https://colab.research.google.com/) â€” Free GPU runtime

## License

This project is for educational purposes as a high-school final project.

